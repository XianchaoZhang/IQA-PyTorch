<!--
# <img align="left" width="100" height="100" src="assets/pyiqa_logo.jpg"> PyTorch Toolbox for Image Quality Assessment
 -->
# <img align="left" width="100" height="100" src="docs/assets/pyiqa_logo.jpg"> ç”¨äºå›¾åƒè´¨é‡è¯„ä¼°çš„ PyTorch å·¥å…·ç®±

<!--
An IQA toolbox with pure python and pytorch. Please refer to [Awesome-Image-Quality-Assessment](https://github.com/chaofengc/Awesome-Image-Quality-Assessment) for a comprehensive survey of IQA methods and download links for IQA datasets.
 -->
ä¸€ä¸ªçº¯ python å’Œ pytorch çš„ IQA å·¥å…·ç®±ã€‚è¯·å‚è€ƒ [Awesome-Image-Quality-Assessment](https://github.com/chaofengc/Awesome-Image-Quality-Assessment) å…¨é¢äº†è§£ IQA æ–¹æ³•å’Œ IQA æ•°æ®é›†ä¸‹è½½é“¾æ¥ã€‚

<a href="https://colab.research.google.com/drive/14J3KoyrjJ6R531DsdOy5Bza5xfeMODi6?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="google colab logo"></a>
[![PyPI](https://img.shields.io/pypi/v/pyiqa)](https://pypi.org/project/pyiqa/)
![visitors](https://visitor-badge.laobi.icu/badge?page_id=chaofengc/IQA-PyTorch)
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/chaofengc/Awesome-Image-Quality-Assessment)
[![Citation](https://img.shields.io/badge/Citation-bibtex-green)](https://github.com/chaofengc/IQA-PyTorch/blob/main/README.md#bookmark_tabs-citation)


<!--
![demo](assets/demo.gif)
 -->
![æ¼”ç¤º](docs/assets/demo.gif)

<!--
- [:open\_book: Introduction](#open_book-introduction)
- [:zap: Quick Start](#zap-quick-start)
  - [Dependencies and Installation](#dependencies-and-installation)
  - [Basic Usage](#basic-usage)
- [:1st\_place\_medal: Benchmark Performances and Model Zoo](#1st_place_medal-benchmark-performances-and-model-zoo)
  - [Results Calibration](#results-calibration)
  - [Performance Evaluation Protocol](#performance-evaluation-protocol)
  - [Benchmark Performance with Provided Script](#benchmark-performance-with-provided-script)
- [:hammer\_and\_wrench: Train](#hammer_and_wrench-train)
  - [Dataset Preparation](#dataset-preparation)
  - [Example Train Script](#example-trai-script)

## :open_book: Introduction
 -->
- [:open\_book: ç®€ä»‹](#open_book-introduction)
- [:zap: å¿«é€Ÿå¯åŠ¨](#zap-quick-start)
  - [ä¾èµ–å…³ç³»å’Œå®‰è£…](#dependency-and-installation)
  - [åŸºæœ¬ç”¨æ³•](#basic-usage)
- [:1st\_place\_medalï¼šåŸºå‡†æ€§èƒ½å’Œ Model Zoo](#1st_place_medal-benchmark-performances-and-model-zoo)
  - [ç»“æœæ ¡å‡†](#results-calibration)
  - [æ€§èƒ½è¯„ä¼°åè®®](#performance-evaluation-protocol)
  - [ä½¿ç”¨æä¾›çš„è„šæœ¬è¿›è¡ŒåŸºå‡†æ€§èƒ½æµ‹è¯•](#benchmark-performance-with-provided-script)
- [:hammer\_and\_wrench: è®­ç»ƒ](#hammer_and_wrench-train)
  - [æ•°æ®é›†å‡†å¤‡](#dataset-preparation)
  - [è®­ç»ƒè„šæœ¬ç¤ºä¾‹](#example-trai-script)
## :open_book: ç®€ä»‹

<!--
This is a image quality assessment toolbox with **pure python and pytorch**. We provide reimplementation of many mainstream full reference (FR) and no reference (NR) metrics (results are calibrated with official matlab scripts if exist). **With GPU acceleration, most of our implementations are much faster than Matlab.** Please refer to the following documents for details:
 -->
è¿™æ˜¯ä¸€ä¸ª **çº¯ python å’Œ pytorch** çš„å›¾åƒè´¨é‡è¯„ä¼°å·¥å…·ç®±ã€‚æˆ‘ä»¬æä¾›äº†è®¸å¤šä¸»æµå…¨å‚è€ƒ (FR) å’Œæ— å‚è€ƒ (NR) æŒ‡æ ‡çš„é‡æ–°å®ç°ï¼ˆç»“æœä½¿ç”¨å®˜æ–¹ matlab è„šæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰è¿›è¡Œæ ¡å‡†ï¼‰ã€‚**é€šè¿‡ GPU åŠ é€Ÿï¼Œæˆ‘ä»¬çš„å¤§éƒ¨åˆ†å®ç°éƒ½æ¯” Matlab å¿«å¾ˆå¤šã€‚**å…·ä½“è¯·å‚è€ƒä»¥ä¸‹æ–‡æ¡£ï¼š

<div align="center">
<!--
ğŸ“¦ [Model Cards](docs/ModelCard.md)  |  ğŸ—ƒï¸ [Dataset Cards](docs/Dataset_Preparation.md)
 -->
ğŸ“¦ [æ¨¡å‹å¡](docs/ModelCard.md) | ğŸ—ƒï¸ [æ•°æ®é›†å¡](docs/Dataset_Preparation.md)
</div>

---

<!--
### :triangular_flag_on_post: Updates/Changelog
- **Aug 15, 2023**. Add `st-lpips` and `laion_aes`. Refer to official repo at [ShiftTolerant-LPIPS](https://github.com/abhijay9/ShiftTolerant-LPIPS) and [improved-aesthetic-predictor](https://github.com/christophschuhmann/improved-aesthetic-predictor)
- **Aug 05, 2023**. Add our work `TOPIQ` with remarkable performance on almost all benchmarks via efficient Resnet50 backbone. Use it with `topiq_fr, topiq_nr, topiq_iaa` for Full-Reference, No-Reference and Aesthetic assessment respectively.
- **March 30, 2023**. Add [URanker](https://github.com/RQ-Wu/UnderwaterRanker) for IQA of under water images.
- **March 29, 2023**. :rotating_light: Hot fix of NRQM & PI.
- **March 25, 2023**. Add TreS, HyperIQA, CNNIQA, CLIPIQA.
- [**More**](docs/history_changelog.md)
 -->
### :triangle_flag_on_post: æ›´æ–°/å˜æ›´æ—¥å¿—
- **2023 å¹´ 8 æœˆ 15 æ—¥**. æ·»åŠ  `st-lpips` å’Œ `laion_aes`ã€‚è¯·å‚é˜… [ShiftTolerant-LPIPS](https://github.com/abhijay9/ShiftTolerant-LPIPS) å’Œ [improved-aesthetic-predictor](https://github.com/christophschuhmann/improved-aesthetic-predictor) çš„å®˜æ–¹å­˜å‚¨åº“
- **2023 å¹´ 8 æœˆ 5 æ—¥**. é€šè¿‡é«˜æ•ˆçš„ Resnet50 ä¸»å¹²ï¼Œæ·»åŠ æˆ‘ä»¬çš„å·¥ä½œ `TOPIQ`ï¼Œåœ¨å‡ ä¹æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­éƒ½å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€‚å°†å…¶ä¸ `topiq_fr, topiq_nr, topiq_iaa` ç»“åˆä½¿ç”¨ï¼Œåˆ†åˆ«è¿›è¡Œå®Œå…¨å‚è€ƒã€æ— å‚è€ƒå’Œå®¡ç¾è¯„ä¼°ã€‚
- **2023 å¹´ 3 æœˆ 30 æ—¥**. æ·»åŠ  [URanker](https://github.com/RQ-Wu/UnderwaterRanker) ç”¨äºæ°´ä¸‹å›¾åƒçš„ IQAã€‚
- **2023 å¹´ 3 æœˆ 29 æ—¥**.  :rotating_lightï¼šNRQM å’Œ PI çš„çƒ­ä¿®å¤/Hot fixã€‚
- **2023 å¹´ 3 æœˆ 25 æ—¥**. æ·»åŠ  TreSã€HyperIQAã€CNNIQAã€CLPIQAã€‚
- [**æ›´å¤š**](docs/history_changelog.md)

---

<!--
## :zap: Quick Start
 -->
## :zapï¼šå¿«é€Ÿå…¥é—¨

### ä¾èµ–å…³ç³»å’Œå®‰è£…
- Ubuntu >= 18.04
- Python >= 3.8
- PyTorch >= 1.12
- Torchvision >= 0.13
- CUDA >= 10.2 (if use GPU)
```
# Install with pip
pip install pyiqa

# Install latest github version
pip uninstall pyiqa # if have older version installed already
pip install git+https://github.com/chaofengc/IQA-PyTorch.git

# Install with git clone
git clone https://github.com/chaofengc/IQA-PyTorch.git
cd IQA-PyTorch
pip install -r requirements.txt
python setup.py develop
```

### åŸºæœ¬ç”¨æ³•

```
import pyiqa
import torch

# list all available metrics
print(pyiqa.list_models())

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

# create metric with default setting
iqa_metric = pyiqa.create_metric('lpips', device=device)
# Note that gradient propagation is disabled by default. set as_loss=True to enable it as a loss function.
iqa_loss = pyiqa.create_metric('lpips', device=device, as_loss=True)

# create metric with custom setting
iqa_metric = pyiqa.create_metric('psnr', test_y_channel=True, color_space='ycbcr').to(device)

# check if lower better or higher better
print(iqa_metric.lower_better)

# example for iqa score inference
# Tensor inputs, img_tensor_x/y: (N, 3, H, W), RGB, 0 ~ 1
score_fr = iqa_metric(img_tensor_x, img_tensor_y)
score_nr = iqa_metric(img_tensor_x)

# img path as inputs.
score_fr = iqa_metric('./ResultsCalibra/dist_dir/I03.bmp', './ResultsCalibra/ref_dir/I03.bmp')

# For FID metric, use directory or precomputed statistics as inputs
# refer to clean-fid for more details: https://github.com/GaParmar/clean-fid
fid_metric = pyiqa.create_metric('fid')
score = fid_metric('./ResultsCalibra/dist_dir/', './ResultsCalibra/ref_dir')
score = fid_metric('./ResultsCalibra/dist_dir/', dataset_name="FFHQ", dataset_res=1024, dataset_split="trainval70k")
```

<!--
#### Example Test script
 -->
#### æµ‹è¯•è„šæœ¬ç¤ºä¾‹

å¸¦æœ‰è¾“å…¥ç›®å½•/å›¾åƒå’Œå‚è€ƒç›®å½•/å›¾åƒçš„ç¤ºä¾‹æµ‹è¯•è„šæœ¬ã€‚
```
# example for FR metric with dirs
python inference_iqa.py -m LPIPS[or lpips] -i ./ResultsCalibra/dist_dir[dist_img] -r ./ResultsCalibra/ref_dir[ref_img]

# example for NR metric with single image
python inference_iqa.py -m brisque -i ./ResultsCalibra/dist_dir/I03.bmp
```

<!--
## :1st_place_medal: Benchmark Performances and Model Zoo
 -->

## :1st_place_medalï¼šåŸºå‡†æ€§èƒ½å’Œ Model Zoo

<!--
### Results Calibration
 -->
### ç»“æœæ ¡å‡†

<!--
Please refer to the [results calibration](./ResultsCalibra/ResultsCalibra.md) to verify the correctness of the python implementations compared with official scripts in matlab or python.
 -->
è¯·å‚è€ƒ[ç»“æœæ ¡å‡†](./ResultsCalibra/ResultsCalibra.md)ï¼Œä¸å®˜æ–¹çš„ matlab æˆ– python è„šæœ¬ç›¸æ¯”ï¼ŒéªŒè¯ python å®ç°çš„æ­£ç¡®æ€§ã€‚

<!--
### Performance Evaluation Protocol
 -->
### æ€§èƒ½è¯„ä¼°åè®®

<!--
**We use official models for evaluation if available.** Otherwise, we use the following settings to train and evaluate different models for simplicity and consistency:
 -->
**å¦‚æœå¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨å®˜æ–¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚**å¦åˆ™ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹è®¾ç½®æ¥è®­ç»ƒå’Œè¯„ä¼°ä¸åŒçš„æ¨¡å‹ï¼Œä»¥å®ç°ç®€å•æ€§å’Œä¸€è‡´æ€§ï¼š

| Metric Type   | Train     | Test                                       | Results                                                  |
| ------------- | --------- | ------------------------------------------ | -------------------------------------------------------- |
| FR            | KADID-10k | CSIQ, LIVE, TID2008, TID2013               | [FR benchmark results](tests/FR_benchmark_results.csv)   |
| NR            | KonIQ-10k | LIVEC, KonIQ-10k (official split), TID2013, SPAQ | [NR benchmark results](tests/NR_benchmark_results.csv)   |
| Aesthetic IQA | AVA       | AVA (official split)                       | [IAA benchmark results](tests/IAA_benchmark_results.csv) |

<!--
Results are calculated with:
- **PLCC without any correction**. Although test time value correction is common in IQA papers, we want to use the original value in our benchmark.
- **Full image single input.** We use multi-patch testing only when it is necessary for the model to work.
 -->
ç»“æœè®¡ç®—å¦‚ä¸‹ï¼š
- **PLCC æœªç»ä»»ä½•æ ¡æ­£/PLCC without any correction**ã€‚å°½ç®¡æµ‹è¯•æ—¶é—´å€¼æ ¡æ­£åœ¨ IQA è®ºæ–‡ä¸­å¾ˆå¸¸è§ï¼Œä½†æˆ‘ä»¬å¸Œæœ›åœ¨åŸºå‡†æµ‹è¯•ä¸­ä½¿ç”¨åŸå§‹å€¼ã€‚
- **å®Œæ•´å›¾åƒå•è¾“å…¥/Full image single inputã€‚** ä»…å½“æ¨¡å‹éœ€è¦æ—¶ï¼Œæˆ‘ä»¬æ‰ä½¿ç”¨å¤šè¡¥ä¸æµ‹è¯•ã€‚

<!--
Basically, we use the largest existing datasets for training, and cross dataset evaluation performance for fair comparison. The following models do not provide official weights, and are retrained by our scripts:
 -->
åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨æœ€å¤§çš„ç°æœ‰æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨è·¨æ•°æ®é›†è¯„ä¼°æ€§èƒ½è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚ä»¥ä¸‹æ¨¡å‹ä¸æä¾›å®˜æ–¹æƒé‡ï¼Œå¹¶ç”±æˆ‘ä»¬çš„è„šæœ¬é‡æ–°è®­ç»ƒï¼š


| Metric Type   | Reproduced Models |
| ------------- | ----------------------------- |
| FR            |                               |
| NR            | `cnniqa`, `dbcnn`, `hyperiqa` |
| Aesthetic IQA | `nima`, `nima-vgg16-ava`      |

<!--
**Important Notes:**
- Due to optimized training process, performance of some retrained approaches may be different with original paper.
- Results of all **retrained models by ours** are normalized to [0, 1] and change to higher better for convenience.
- Results of KonIQ-10k, AVA are both tested with official split.
- NIMA is only applicable to AVA dataset now. We use `inception_resnet_v2` for default `nima`.
- MUSIQ is not included in the IAA benchmark because we do not have train/split information of the official model.
 -->
**é‡è¦æç¤ºï¼š**
- ç”±äºä¼˜åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œä¸€äº›é‡æ–°è®­ç»ƒæ–¹æ³•çš„æ€§èƒ½å¯èƒ½ä¸åŸå§‹è®ºæ–‡ä¸åŒã€‚
- æˆ‘ä»¬çš„æ‰€æœ‰**é‡æ–°è®­ç»ƒæ¨¡å‹**çš„ç»“æœéƒ½æ ‡å‡†åŒ–ä¸º [0, 1]ï¼Œæ–¹ä¾¿èµ·è§ï¼Œæ›´æ”¹ä¸ºæ›´é«˜æ›´å¥½ã€‚
- KonIQ-10kã€AVA çš„ç»“æœå‡ç»è¿‡å®˜æ–¹æ‹†åˆ†æµ‹è¯•ã€‚
- NIMA ç›®å‰ä»…é€‚ç”¨äº AVA æ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨ `inception_resnet_v2` ä½œä¸ºé»˜è®¤çš„ `nima`ã€‚
- MUSIQ æœªåŒ…å«åœ¨ IAA åŸºå‡†æµ‹è¯•ä¸­ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰å®˜æ–¹æ¨¡å‹çš„è®­ç»ƒ/æ‹†åˆ†ä¿¡æ¯ã€‚

<!--
### Benchmark Performance with Provided Script
 -->
### ä½¿ç”¨æä¾›çš„è„šæœ¬è¿›è¡ŒåŸºå‡†æ€§èƒ½æµ‹è¯•

ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹è„šæœ¬ï¼Œç”¨äºè·å–ä¸åŒæ•°æ®é›†çš„æ€§èƒ½åŸºå‡†ï¼š
```
# NOTE: this script will test ALL specified metrics on ALL specified datasets
# Test default metrics on default datasets
python benchmark_results.py -m psnr ssim -d csiq tid2013 tid2008

# Test with your own options
python benchmark_results.py -m psnr --data_opt options/example_benchmark_data_opts.yml

python benchmark_results.py --metric_opt options/example_benchmark_metric_opts.yml tid2013 tid2008

python benchmark_results.py --metric_opt options/example_benchmark_metric_opts.yml --data_opt options/example_benchmark_data_opts.yml
```

<!--
## :hammer_and_wrench: Train
 -->

## :hammer_and_wrench: è®­ç»ƒ

<!--
### Dataset Preparation
 -->
### æ•°æ®é›†å‡†å¤‡

<!--
- You only need to unzip downloaded datasets from official website without any extra operation. And then make soft links of these dataset folder under `datasets/` folder. Download links are provided in [Awesome-Image-Quality-Assessment](https://github.com/chaofengc/Awesome-Image-Quality-Assessment).
- We provide common interface to load these datasets with the prepared meta information files and train/val/test split files, which can be downloaded from [download_link](https://github.com/chaofengc/IQA-PyTorch/releases/download/v0.1-weights/meta_info.tgz) and extract them to `datasets/` folder.
 -->
- æ‚¨åªéœ€ä»å®˜æ–¹ç½‘ç«™è§£å‹ä¸‹è½½çš„æ•°æ®é›†ï¼Œæ— éœ€ä»»ä½•é¢å¤–æ“ä½œã€‚ç„¶ååœ¨ `datasets/` æ–‡ä»¶å¤¹ä¸‹åˆ›å»ºè¿™äº›æ•°æ®é›†æ–‡ä»¶å¤¹çš„è½¯é“¾æ¥ã€‚[Awesome-Image-Quality-Assessment](https://github.com/chaofengc/Awesome-Image-Quality-Assessment) ä¸­æä¾›äº†ä¸‹è½½é“¾æ¥ã€‚
- æˆ‘ä»¬æä¾›é€šç”¨æ¥å£æ¥åŠ è½½è¿™äº›æ•°æ®é›†ä»¥åŠå‡†å¤‡å¥½çš„å…ƒä¿¡æ¯æ–‡ä»¶å’Œ train/val/test åˆ†å‰²æ–‡ä»¶ï¼Œè¿™äº›æ–‡ä»¶å¯ä»¥ä»[download_link](https://github.com/chaofengc/IQA-PyTorch/releases/download/v0.1-weights/meta_info.tgz) ä¸‹è½½å¹¶å°†å®ƒä»¬æå–åˆ° `datasets/` æ–‡ä»¶å¤¹ã€‚

<!--
You may also use the following commands:
 -->
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```
mkdir datasets && cd datasets

# make soft links of your dataset
ln -sf your/dataset/path datasetname

# download meta info files and train split files
wget https://github.com/chaofengc/IQA-PyTorch/releases/download/v0.1-weights/meta_info.tgz
tar -xvf data_info_files.tgz
```

<!--
Examples to specific dataset options can be found in `./options/default_dataset_opt.yml`. Details of the dataloader inferface and meta information files can be found in [Dataset Preparation](docs/Dataset_Preparation.md)
 -->
ç‰¹å®šæ•°æ®é›†é…ç½®çš„ç¤ºä¾‹å¯ä»¥åœ¨ `./options/default_dataset_opt.yml` ä¸­æ‰¾åˆ°ã€‚ dataloader æ¥å£å’Œå…ƒä¿¡æ¯æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨[æ•°æ®é›†å‡†å¤‡](docs/Dataset_Preparation.md)ä¸­æ‰¾åˆ°ã€‚

<!--
### Example Train Script
 -->
### è®­ç»ƒè„šæœ¬ç¤ºä¾‹

åœ¨ LIVEChallenge æ•°æ®é›†ä¸Šè®­ç»ƒ DBCNN çš„ç¤ºä¾‹
```
# train for single experiment
python pyiqa/train.py -opt options/train/DBCNN/train_DBCNN.yml

# train N splits for small datasets
python pyiqa/train_nsplits.py -opt options/train/DBCNN/train_DBCNN.yml
```

<!--
## :beers: Contribution
 -->

## :beers: è´¡çŒ®

<!--
Any contributions to this repository are greatly appreciated. Please follow the [contribution instructions](docs/Instruction.md) for contribution guidance.
 -->
éå¸¸æ„Ÿè°¢å¯¹æ­¤å­˜å‚¨åº“çš„ä»»ä½•è´¡çŒ®ã€‚è¯·æŒ‰ç…§[è´¡çŒ®è¯´æ˜](docs/Instruction.md) è·å–è´¡çŒ®æŒ‡å¯¼ã€‚

<!--
## :scroll: License
 -->
## :scroll: è®¸å¯è¯

<!--
This work is licensed under a [NTU S-Lab License](https://github.com/chaofengc/IQA-PyTorch/blob/main/LICENSE_NTU-S-Lab) and <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
 -->
æœ¬ä½œå“å·²è·å¾— [NTU S-Lab è®¸å¯è¯](https://github.com/chaofengc/IQA-PyTorch/blob/main/LICENSE_NTU-S-Lab) å’Œ <a rel="license" href="http ://creativecommons.org/licenses/by-nc-sa/4.0/">çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯</a>ã€‚

<!--
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>
 -->
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="çŸ¥è¯†å…±äº«è®¸å¯è¯" style="border-width:0" src= â€œhttps://i.creativecommons.org/l/by-nc-sa/4.0/88x31.pngâ€/></a>

<!--
## :bookmark_tabs: Citation
 -->
## :bookmark_tabs: å¼•æ–‡

<!--
If you find our codes helpful to your research, please consider to use the following citation:
 -->
å¦‚æœæ‚¨å‘ç°æˆ‘ä»¬çš„ä»£ç å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ä½¿ç”¨ä»¥ä¸‹å¼•ç”¨ï¼š

```
@misc{pyiqa,
  title={{IQA-PyTorch}: PyTorch Toolbox for Image Quality Assessment},
  author={Chaofeng Chen and Jiadi Mo},
  year={2022},
  howpublished = "[Online]. Available: \url{https://github.com/chaofengc/IQA-PyTorch}"
}
```

å¦‚æœå¯¹æ‚¨æœ‰ç”¨ï¼Œè¿˜è¯·è€ƒè™‘å¼•ç”¨æˆ‘ä»¬çš„æ–°å·¥ä½œ `TOPIQ`ï¼š
```
@misc{chen2023topiq,
      title={TOPIQ: A Top-down Approach from Semantics to Distortions for Image Quality Assessment},
      author={Chaofeng Chen and Jiadi Mo and Jingwen Hou and Haoning Wu and Liang Liao and Wenxiu Sun and Qiong Yan and Weisi Lin},
      year={2023},
      eprint={2308.03060},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

<!--
## :heart: Acknowledgement
 -->

## :heart: è‡´è°¢

<!--
The code architecture is borrowed from [BasicSR](https://github.com/xinntao/BasicSR). Several implementations are taken from: [IQA-optimization](https://github.com/dingkeyan93/IQA-optimization), [Image-Quality-Assessment-Toolbox](https://github.com/RyanXingQL/Image-Quality-Assessment-Toolbox), [piq](https://github.com/photosynthesis-team/piq), [piqa](https://github.com/francois-rozet/piqa), [clean-fid](https://github.com/GaParmar/clean-fid)
 -->
ä»£ç æ¶æ„å€Ÿé‰´äº† [BasicSR](https://github.com/xinntao/BasicSR)ã€‚å‡ ç§å®ç°å–è‡ªï¼š[IQA-optimization](https://github.com/dingkeyan93/IQA-optimization)ã€[Image-Quality-Assessment-Toolbox](https://github.com/RyanXingQL/Image-Quality-Assessment-Toolbox)ï¼Œ[piq](https://github.com/photosynthesis-team/piq), [piqa](https://github.com/francois-rozet/piqa), [clean-fid](https://github.com/GaParmar/clean-fid)

<!--
We also thanks the following public repositories: [MUSIQ](https://github.com/google-research/google-research/tree/master/musiq), [DBCNN](https://github.com/zwx8981/DBCNN-PyTorch), [NIMA](https://github.com/kentsyx/Neural-IMage-Assessment), [HyperIQA](https://github.com/SSL92/hyperIQA), [CNNIQA](https://github.com/lidq92/CNNIQA), [WaDIQaM](https://github.com/lidq92/WaDIQaM), [PieAPP](https://github.com/prashnani/PerceptualImageError), [paq2piq](https://github.com/baidut/paq2piq), [MANIQA](https://github.com/IIGROUP/MANIQA)
 -->
æˆ‘ä»¬è¿˜æ„Ÿè°¢ä»¥ä¸‹å…¬å…±å­˜å‚¨åº“ï¼š[MUSIQ](https://github.com/google-research/google-research/tree/master/musiq), [DBCNN](https://github.com/zwx8981/DBCNN-PyTorch), [NIMA](https://github.com/kentsyx/Neural-IMage-Assessment), [HyperIQA](https://github.com/SSL92/hyperIQA), [CNNIQA](https://github.com/lidq92/CNNIQA), [WaDIQaM](https://github.com/lidq92/WaDIQaM), [PieAPP](https://github.com/prashnani/PerceptualImageError), [paq2piq](https://github.com/baidut/paq2piq), [MANIQA](https://github.com/IIGROUP/MANIQA)

<!--
## :e-mail: Contact
 -->
## :e-mail: è”ç³»æ–¹å¼

<!--
If you have any questions, please email `chaofenghust@gmail.com`
 -->
å¦‚æœæ‚¨æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·å‘é€ç”µå­é‚®ä»¶è‡³ `chaofenghust@gmail.com`